{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ba5f36-619c-4805-86f0-664d869427e7",
   "metadata": {},
   "source": [
    "## Colchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaf277a-cedf-471f-ae58-5f9a107f6446",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using DrugBank logs we do following:\n",
    "\n",
    "1. preprocess format of Drugbank logs\n",
    "\n",
    "2. parse the logs\n",
    "\n",
    "3. Calculate C and I (complex queries)\n",
    "\n",
    "4. find the queries with multiple prefixes(federation)\n",
    "\n",
    "5. find the queries having predicate: { type: 'path'} and pathType: '+','*' (star and propersty path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0978803-77d2-4f10-9cf8-7e053de7c360",
   "metadata": {},
   "source": [
    "Query selection\n",
    "https://docs.google.com/document/d/1I8XCZ-wnG1L89lGQxxbLUShUHPIBe9gOHu58aoWy1rc/edit\n",
    "\n",
    "Complexity (# of triple patterns, )\n",
    "\n",
    "Path join (eg., subject of a triple can be object) and star joins\n",
    "\n",
    "Linkedness .. (use of multiple datasets, # of different namespace (ignoring rdf: rdfs:))\n",
    "\n",
    "Use of Expensive SPARQL operations (Aggregate functions, Group By (Not an issue), ORDER BY (Easy), FILTER (Low), FILTER with regex or substring (High), OPTIONAL (High))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876c7e7-6522-4196-817f-aceb382e59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparqlQuery = '''select * where{\n",
    "  ?x <example:www/mbox> <mailto:alice@example> .\n",
    "  ?x <foaf:knows>+/<foaf:name> ?name .\n",
    " }'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca7389-c7ed-451b-81c5-7e9ab7520d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  queryType: 'SELECT',\n",
    "  variables: '*',\n",
    "  where: [\n",
    "    {\n",
    "      type: 'bgp',\n",
    "      triples: [\n",
    "        {\n",
    "          subject: Variable { termType: 'Variable', value: 'x' },\n",
    "          predicate: NamedNode {\n",
    "            termType: 'NamedNode',\n",
    "            value: 'example:www/mbox'\n",
    "          },\n",
    "          object: NamedNode {\n",
    "            termType: 'NamedNode',\n",
    "            value: 'mailto:alice@example'\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          subject: Variable { termType: 'Variable', value: 'x' },\n",
    "          predicate: {\n",
    "            type: 'path',\n",
    "            pathType: '/',\n",
    "            items: [\n",
    "              {\n",
    "                type: 'path',\n",
    "                pathType: '+',\n",
    "                items: [\n",
    "                  NamedNode {\n",
    "                    termType: 'NamedNode',\n",
    "                    value: 'foaf:knows'\n",
    "                  }\n",
    "                ]\n",
    "              },\n",
    "              NamedNode { termType: 'NamedNode', value: 'foaf:name' }\n",
    "            ]\n",
    "          },\n",
    "          object: Variable { termType: 'Variable', value: 'name' }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  type: 'query',\n",
    "  prefixes: {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f528d3d-17b2-4243-9993-de688b74137c",
   "metadata": {},
   "source": [
    "# Parse queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9a6d4-c6a7-49dd-b5da-7b8436f5cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require('fs');\n",
    "const SparqlParser = require('sparqljs').Parser;\n",
    "const csvParser = require('csv-parser');\n",
    "const { unparse } = require('papaparse');\n",
    "const { Worker, isMainThread, parentPort } = require('worker_threads');\n",
    "const pMap = require('p-map');\n",
    "\n",
    "// Create a new instance of the SPARQL parser\n",
    "const parser = new SparqlParser();\n",
    "\n",
    "// Function to execute a single SPARQL query and return the result\n",
    "async function executeQuery(query) {\n",
    "  try {\n",
    "    const parsedQuery = parser.parse(query);\n",
    "    // Here, you can execute the parsed query using your preferred SPARQL endpoint or RDF library\n",
    "    // For the sake of example, we will just return the parsed query as a string\n",
    "    return JSON.stringify(parsedQuery);\n",
    "  } catch (error) {\n",
    "    console.error('Error parsing query:', query);\n",
    "    return 'Error parsing the query.';\n",
    "  }\n",
    "}\n",
    "\n",
    "async function main() {\n",
    "  // Read SPARQL queries from input CSV file and execute them in parallel\n",
    "  const inputCsvFile = 'prefix_unique_bio2rdf_sparql_logs.csv';\n",
    "  const outputCsvFile = 'translated_prefix_unique_bio2rdf_sparql_logs.csv';\n",
    "\n",
    "  const csvData = [];\n",
    "\n",
    "  // Step 1: Count the total number of queries in the input CSV file\n",
    "  let totalQueries = 0;\n",
    "  fs.createReadStream(inputCsvFile)\n",
    "    .pipe(csvParser())\n",
    "    .on('data', () => {\n",
    "      totalQueries++;\n",
    "    })\n",
    "    .on('end', () => {\n",
    "      console.log(`Total SPARQL queries to process: ${totalQueries}`);\n",
    "    });\n",
    "\n",
    "  const workerFunction = async (row) => {\n",
    "    const sparqlQuery = row['query']; // Adjust the column name as per your input CSV format\n",
    "    const parsedQuery = await executeQuery(sparqlQuery);\n",
    "\n",
    "    // Add the results to the csvData array\n",
    "    csvData.push({ Query: sparqlQuery, Parsed_Query: parsedQuery });\n",
    "\n",
    "    // Calculate the completion percentage and show the progress\n",
    "    const completionPercentage = (csvData.length / totalQueries) * 100;\n",
    "    console.log(`Progress: ${completionPercentage.toFixed(2)}%`);\n",
    "\n",
    "    return { Query: sparqlQuery, Parsed_Query: parsedQuery };\n",
    "  };\n",
    "\n",
    "  // Use pMap to parallelize the execution of queries\n",
    "  const workerCount = 4; // Adjust the number of worker threads as per your CPU cores\n",
    "  await pMap(\n",
    "    fs.createReadStream(inputCsvFile).pipe(csvParser()),\n",
    "    workerFunction,\n",
    "    { concurrency: workerCount }\n",
    "  );\n",
    "\n",
    "  // Write the results to output CSV file\n",
    "  const csv = unparse(csvData, { header: true, delimiter: ',' });\n",
    "  fs.writeFileSync(outputCsvFile, csv);\n",
    "\n",
    "  console.log('All SPARQL queries executed and results written to output CSV file.');\n",
    "}\n",
    "\n",
    "if (isMainThread) {\n",
    "  main();\n",
    "} else {\n",
    "  parentPort.once('message', (message) => {\n",
    "    parentPort.postMessage(workerFunction(message));\n",
    "  });\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
