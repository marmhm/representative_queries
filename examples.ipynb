{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a91c40-1123-4ea6-ae17-682a883fb456",
   "metadata": {},
   "source": [
    "## We remove all queries from the part starts with any of these:\n",
    "\n",
    "&format\n",
    "\n",
    "&timeout\n",
    "\n",
    "&debug\n",
    "\n",
    "&run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5c977751-576e-4ad5-8471-44ee5679279d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5259/2559306569.py:18: DtypeWarning: Columns (1,2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def remove_keywords(query):\n",
    "    # List of keywords to remove\n",
    "    keywords = [\"&format\", \"&timeout\", \"&debug\", \"&run\"]\n",
    "\n",
    "    # Regular expression to find any of the keywords in the query\n",
    "    regex = '|'.join(re.escape(keyword) for keyword in keywords)\n",
    "\n",
    "    # Remove the matched part of the query and everything after it\n",
    "    query = re.sub(rf'({regex}).*', '', query)\n",
    "\n",
    "    return query.strip()\n",
    "\n",
    "def process_csv(input_file, output_file):\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Process the \"query\" column using the remove_keywords function\n",
    "    df['query'] = df['query'].apply(remove_keywords)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = \"bio2rdf_sparql_logs_processed_01-2019_to_07-2021.csv\"\n",
    "    output_csv_file = \"removedformats_bio2rdf_logs.csv\"\n",
    "    process_csv(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fda428-d9c7-4b72-b4e7-8cd9980bc6d0",
   "metadata": {},
   "source": [
    "## We extract unique queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a940dc10-634a-4b1e-990e-25e5417a9cae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5259/1392910086.py:6: DtypeWarning: Columns (1,2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique queries extracted and written to unique_bio2rdf_sparql_logs.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_unique_queries(input_file, output_file):\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        # Extract unique values from the 'query' column\n",
    "        unique_queries = df['query'].unique()\n",
    "\n",
    "        # Create a new DataFrame with the unique queries\n",
    "        unique_queries_df = pd.DataFrame({'query': unique_queries})\n",
    "\n",
    "        # Write the unique queries DataFrame to a new CSV file\n",
    "        unique_queries_df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(\"Unique queries extracted and written to\", output_file)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Input CSV file not found.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "# Replace 'input_file.csv' with the name of your CSV file and 'output_file.csv' with the desired output filename\n",
    "extract_unique_queries('removedformats_bio2rdf_logs.csv', 'unique_bio2rdf_sparql_logs.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6eca15-99d8-4028-95db-13b4a1e6084f",
   "metadata": {},
   "source": [
    "## we add following list to all queries:\n",
    "\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "\n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "\n",
    "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "\n",
    "PREFIX schema: <http://schema.org/>\n",
    "\n",
    "PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea88567-b74d-4af4-a1cb-35d5b91fc0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_prefix(query):\n",
    "    # The common prefix to add to each query\n",
    "    prefix = \"\"\"PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "PREFIX schema: <http://schema.org/>\n",
    "PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "\n",
    "\"\"\"\n",
    "    return prefix + query\n",
    "\n",
    "def add_prefix_to_csv(input_file, output_file):\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Add the prefix to the \"query\" column using the add_prefix function\n",
    "    df['query'] = df['query'].apply(add_prefix)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = \"unique_bio2rdf_sparql_logs.csv\"  # Replace with the path to your input CSV file\n",
    "    output_csv_file = \"prefix_unique_bio2rdf_sparql_logs.csv\"  # Replace with the path to your output CSV file\n",
    "    add_prefix_to_csv(input_csv_file, output_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ff815-a46c-4656-a338-a8b8214502d3",
   "metadata": {},
   "source": [
    "## We count subjects to calculate complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "58262cc3-dbe4-4830-8c79-a12d5d8465b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_subject_complexity(parsed_query):\n",
    "    return parsed_query.count(\"subject\")\n",
    "\n",
    "# Replace 'input.csv' with the name of your input CSV file and 'output.csv' with the desired output CSV file name\n",
    "input_file = 'translated_queries.csv'\n",
    "output_file = 'complexity.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Create a new column 'complexity' by counting the occurrences of 'subject' in 'parsed_query'\n",
    "df['complexity'] = df['Parsed_Query'].apply(count_subject_complexity)\n",
    "\n",
    "# Create a new DataFrame with 'query' and 'complexity' columns\n",
    "result_df = df[['Query','Parsed_Query', 'complexity']]\n",
    "\n",
    "# Save the result DataFrame to a new CSV file\n",
    "result_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4b72d515-5c92-4dde-a59a-20531ccf4200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'Variable' terms after 'where': 4\n",
      "Number of 'NamedNode' terms: 4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def count_variable_and_namednode_terms(json_text):\n",
    "    parsed_json = json.loads(json_text)\n",
    "    where_clause = parsed_json.get('where', [])\n",
    "\n",
    "    variable_count = 0\n",
    "    namednode_count = 0\n",
    "\n",
    "    for clause in where_clause:\n",
    "        if 'triples' in clause:\n",
    "            for triple in clause['triples']:\n",
    "                if 'subject' in triple and 'termType' in triple['subject'] and triple['subject']['termType'] == 'Variable':\n",
    "                    variable_count += 1\n",
    "                if 'predicate' in triple and 'termType' in triple['predicate'] and triple['predicate']['termType'] == 'NamedNode':\n",
    "                    namednode_count += 1\n",
    "                if 'object' in triple and 'termType' in triple['object'] and triple['object']['termType'] == 'Variable':\n",
    "                    variable_count += 1\n",
    "                if 'object' in triple and 'termType' in triple['object'] and triple['object']['termType'] == 'NamedNode':\n",
    "                    namednode_count += 1\n",
    "\n",
    "    return variable_count, namednode_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = '''\n",
    "    {\n",
    "      \"queryType\": \"SELECT\",\n",
    "      \"variables\": [\n",
    "        {\n",
    "          \"termType\": \"Variable\",\n",
    "          \"value\": \"p\"\n",
    "        },\n",
    "        {\n",
    "          \"termType\": \"Variable\",\n",
    "          \"value\": \"c\"\n",
    "        }\n",
    "      ],\n",
    "      \"where\": [\n",
    "        {\n",
    "          \"type\": \"bgp\",\n",
    "          \"triples\": [\n",
    "            {\n",
    "              \"subject\": {\n",
    "                \"termType\": \"Variable\",\n",
    "                \"value\": \"p\"\n",
    "              },\n",
    "              \"predicate\": {\n",
    "                \"termType\": \"NamedNode\",\n",
    "                \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\"\n",
    "              },\n",
    "              \"object\": {\n",
    "                \"termType\": \"NamedNode\",\n",
    "                \"value\": \"http://dbpedia.org/ontology/Artist\"\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"subject\": {\n",
    "                \"termType\": \"Variable\",\n",
    "                \"value\": \"p\"\n",
    "              },\n",
    "              \"predicate\": {\n",
    "                \"termType\": \"NamedNode\",\n",
    "                \"value\": \"http://dbpedia.org/ontology/birthPlace\"\n",
    "              },\n",
    "              \"object\": {\n",
    "                \"termType\": \"Variable\",\n",
    "                \"value\": \"c\"\n",
    "              }\n",
    "            },\n",
    "            {\n",
    "              \"subject\": {\n",
    "                \"termType\": \"Variable\",\n",
    "                \"value\": \"c\"\n",
    "              },\n",
    "              \"predicate\": {\n",
    "                \"termType\": \"NamedNode\",\n",
    "                \"value\": \"http://xmlns.com/foaf/0.1/name\"\n",
    "              },\n",
    "              \"object\": {\n",
    "                \"termType\": \"Literal\",\n",
    "                \"value\": \"York\",\n",
    "                \"language\": \"en\",\n",
    "                \"datatype\": {\n",
    "                  \"termType\": \"NamedNode\",\n",
    "                  \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#langString\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"type\": \"query\",\n",
    "      \"prefixes\": {\n",
    "        \"dbpedia-owl\": \"http://dbpedia.org/ontology/\"\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    variable_count, namednode_count = count_variable_and_namednode_terms(text)\n",
    "    print(\"Number of 'Variable' terms after 'where':\", variable_count)\n",
    "    print(\"Number of 'NamedNode' terms:\", namednode_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "aaa6bbcd-e044-4226-a9cf-4d80baf225f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Variable Count: 5\n",
      "Total Named Node Count: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "text = '''\n",
    "    {\n",
    "      \"queryType\": \"SELECT\",\n",
    "      \"variables\": \"x y\",\n",
    "      \"where\": [\n",
    "        {\n",
    "          \"type\": \"bgp\",\n",
    "          \"triples\": [\n",
    "            {\n",
    "              \"subject\": { \"termType\": \"Variable\", \"value\": \"x\" },\n",
    "              \"predicate\": { \"termType\": \"NamedNode\", \"value\": \"http://www.w3.org/1999/02/22-rdf-syntax-ns#type\" },\n",
    "              \"object\": { \"termType\": \"Variable\", \"value\": \"y\" }\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"group\",\n",
    "          \"patterns\": [\n",
    "            {\n",
    "              \"queryType\": \"SELECT\",\n",
    "              \"variables\": [ { \"termType\": \"Variable\", \"value\": \"y\" } ],\n",
    "              \"where\": [\n",
    "                {\n",
    "                  \"type\": \"bgp\",\n",
    "                  \"triples\": [\n",
    "                    {\n",
    "                      \"subject\": { \"termType\": \"Variable\", \"value\": \"y\" },\n",
    "                      \"predicate\": { \"termType\": \"Variable\", \"value\": \"o\" },\n",
    "                      \"object\": { \"termType\": \"Variable\", \"value\": \"d\" }\n",
    "                    }\n",
    "                  ]\n",
    "                }\n",
    "              ],\n",
    "              \"type\": \"query\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"type\": \"query\",\n",
    "      \"prefixes\": {}\n",
    "    }\n",
    "    '''\n",
    "\n",
    "# Parse the JSON text into a Python dictionary\n",
    "data = json.loads(text)\n",
    "\n",
    "# Function to count variables and named nodes in the bgp section\n",
    "def count_in_bgp(bgp_data):\n",
    "    variable_count = 0\n",
    "    named_node_count = 0\n",
    "\n",
    "    if \"triples\" in bgp_data:\n",
    "        triples = bgp_data[\"triples\"]\n",
    "        for triple in triples:\n",
    "            if triple[\"subject\"][\"termType\"] == \"Variable\":\n",
    "                variable_count += 1\n",
    "            elif triple[\"subject\"][\"termType\"] == \"NamedNode\":\n",
    "                named_node_count += 1\n",
    "            if triple[\"predicate\"][\"termType\"] == \"Variable\":\n",
    "                variable_count += 1\n",
    "            elif triple[\"predicate\"][\"termType\"] == \"NamedNode\":\n",
    "                named_node_count += 1\n",
    "            if triple[\"object\"][\"termType\"] == \"Variable\":\n",
    "                variable_count += 1\n",
    "            elif triple[\"object\"][\"termType\"] == \"NamedNode\":\n",
    "                named_node_count += 1\n",
    "\n",
    "    return variable_count, named_node_count\n",
    "\n",
    "# Initialize counts\n",
    "total_variable_count = 0\n",
    "total_named_node_count = 0\n",
    "\n",
    "# Traverse through all \"bgp\" sections and count variables and named nodes in bgps\n",
    "def traverse(data):\n",
    "    global total_variable_count, total_named_node_count\n",
    "    if isinstance(data, dict):\n",
    "        if \"type\" in data and data[\"type\"] == \"bgp\":\n",
    "            variable_count, named_node_count = count_in_bgp(data)\n",
    "            total_variable_count += variable_count\n",
    "            total_named_node_count += named_node_count\n",
    "        for key, value in data.items():\n",
    "            traverse(value)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            traverse(item)\n",
    "\n",
    "# Start traversing from the main JSON dictionary\n",
    "traverse(data)\n",
    "\n",
    "# Print the results\n",
    "print(\"Total Variable Count:\", total_variable_count)\n",
    "print(\"Total Named Node Count:\", total_named_node_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ca2ca7-4659-4449-b632-aba488a34d94",
   "metadata": {},
   "source": [
    "## We calculate Informativness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fa798351-87c6-4ab6-a4e7-8eec06ef2b87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing JSON in row: {'Query': 'select * where {[] a ?Concept} LIMIT 100&format=text/html&timeout=0&debug=on&run= Run Query', 'Parsed_Query': 'Error parsing the query.', 'complexity': '0'}\n",
      "Error parsing JSON in row: {'Query': 'With the advantages of utilizing a job recruiter weighed towards the disadvantages you\\'ll discover that most often using a job headhunter is not \\nthe very best option for you or your organization. Whhen Mr.\\nDelbart’s job att Orajge CH turned redundant, he was employed \\nas Head of Support for Transport Publics Genevois, a personal \\ncompany offering transport facilities to the public.\\nLet me let you know slightly about Mr. Delbart’s career before analyzing \\nthe common principles that it is best to observe.\\nSince 2009, Betts Recruiting has been serving to veterans \\nmake career dreams come true and serving to corporations scale \\nfor growth. At the time, he was inquisitive about having a technical profession related to Information Technology (IT) improvement and maintenance.\\n\\nHaving learned about the value of an MBA program from a colleague, he next started \\nfinding out at Rushmore University, where he centered on the idea and follow of promoting, sustainable growth, the Internet, \\nandd IT. An unexpected benefit of his MBA program was learning the right way to be simpler in researching and understanding \\nnew topics on his personal, making iit easier for \\nhim to prokceed his schooling after graduating.Seeking much more variety and scope for his profession, he joined with a associate to launch a \\nweb design agency.\\n\\nRight now, it haas yet to point out up in current government efficiency knowledge,\\nannd truly, tthe latest jobs report showed even a slight drop within the number of financial professionals.\\nEven if the person you’re talking to doesn’t endure from that \\nspecific problem, by specializing in problems moderately than some bland recitation about what you \"do\", you dramatically increase \\nthe chance that they will remember you. You will want to think aout your consolation level with different coaching strategies, comparable to speaking in entrance of \\nmassive groups, one-on-one coaching, talking over the telephone, corresponding \\nby electronic mail or another platform, and find the supply platform that works finest for you.\\nThe point is that if you want to get someone’s attention yyou should concentrate on the \\nproblems that you simply clear up. In some unspecified time in the future \\nthe one distinction between you and the latest employee \\nis thaqt you\\'ve got more experience. If a headhunter contacts you about a specific job, the primary question that \\nyou need to pose to him or her is \"How many people have you positioned with this employer, the one you might be contacting me about, previously 12 months? They\\'re lawyers or attorney recruiters who help the unemployed land a job.\\n\\nWho are you aware who is aware of somebody who knows some else at the company you might be serious about? Thus, somebody who began out handling a single aspect of the nationwide actions for the company’s operations would finally handle only a small portion of that original side. Iris posted on the Gmail Help forum instructions for determining whether or not somebody is forwarding mail to the account. Next, he left Swisscom to join a new cellular vendor, Orange CH, which allowed him the opportunity to assist construct a company from scratch. The subsequent method and likewise the newest solution to recruit help is to make use of a web-based service. As I considered his career, I might clearly see a method out for people who discover themselves in an excessively circumscribed, ever-narrowing job in an more and more specialized industry where little differentiation seems to be attainable. All that’s left for us to do now could be to send out our best agents out to apprehend the scofflaws, now that we\\'ve got uncovered their nefarious plot.\\n\\nShe might attempt to get attention by saying, \"I’m \\na marketing advisor.\" In a single ear and out the other. However, every reply follows the same format off hooking consideration by focusing on an issue. Use an alternate digital or paper format. Ask your self: Do you really need to use Executive Search Services to search out your nnew leader? Instead, at this time\\'s\\' most successful businesses are turning to skilled headhunters tto finish their govt search in China. Search for headhunter positions on websites like Glassdoor, Indeed, or LinkedIn. Also feedback in a LinkedIn (or different social media) saying \\'I am involved\\' or \\'I qualify\\' do not yield any outcomes. While the Deell method is great forr handling progress, it can be stultifying for the individuals whose jobs grow to be smaller and smaller in scope as the company’s imension will increase. Right now, there are still more jobs which are cut than new hires. Whether or not you ever have that type of alternative, you\\'ll turn out to be effectively prepared to take on thrilling new roles and to be mire profitable on the older ones.&format=text/html&timeout=0&debug=on&run=Go', 'Parsed_Query': 'Error parsing the query.', 'complexity': '0'}\n"
     ]
    }
   ],
   "source": [
    " import json\n",
    "import csv\n",
    "\n",
    "# Function to count variables and named nodes in the bgp section\n",
    "def count_in_bgp(bgp_data):\n",
    "    variable_count = 0\n",
    "    named_node_count = 0\n",
    "\n",
    "    if \"triples\" in bgp_data:\n",
    "        triples = bgp_data[\"triples\"]\n",
    "        for triple in triples:\n",
    "            if triple[\"subject\"][\"termType\"] == \"Variable\":\n",
    "                variable_count += 1\n",
    "            elif triple[\"subject\"][\"termType\"] == \"NamedNode\":\n",
    "                named_node_count += 1\n",
    "            if triple[\"predicate\"][\"termType\"] == \"Variable\":\n",
    "                variable_count += 1\n",
    "            elif triple[\"predicate\"][\"termType\"] == \"NamedNode\":\n",
    "                named_node_count += 1\n",
    "            if triple[\"object\"][\"termType\"] == \"Variable\":\n",
    "                variable_count += 1\n",
    "            elif triple[\"object\"][\"termType\"] == \"NamedNode\":\n",
    "                named_node_count += 1\n",
    "\n",
    "    return variable_count, named_node_count\n",
    "\n",
    "# Function to calculate informativeness and update CSV rows\n",
    "def calculate_informativeness(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames + ['Informativeness']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            parsed_query = row['Parsed_Query']\n",
    "\n",
    "            try:\n",
    "                data = json.loads(parsed_query)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing JSON in row: {row}\")\n",
    "                continue\n",
    "\n",
    "            total_variable_count = 0\n",
    "            total_named_node_count = 0\n",
    "\n",
    "            # Traverse through all \"bgp\" sections and count variables and named nodes in bgps\n",
    "            def traverse(data):\n",
    "                nonlocal total_variable_count, total_named_node_count\n",
    "                if isinstance(data, dict):\n",
    "                    if \"type\" in data and data[\"type\"] == \"bgp\":\n",
    "                        variable_count, named_node_count = count_in_bgp(data)\n",
    "                        total_variable_count += variable_count\n",
    "                        total_named_node_count += named_node_count\n",
    "                    for key, value in data.items():\n",
    "                        traverse(value)\n",
    "                elif isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        traverse(item)\n",
    "\n",
    "            # Start traversing from the main JSON dictionary\n",
    "            traverse(data)\n",
    "\n",
    "            # Update the row with informativeness values\n",
    "            informativeness = total_named_node_count / (total_variable_count + total_named_node_count)\n",
    "\n",
    "            row['Informativeness'] = f'{informativeness}'\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Provide the input and output file paths\n",
    "input_csv_file = 'complexity.csv'\n",
    "output_csv_file = 'informativness.csv'\n",
    "\n",
    "# Call the function to calculate informativeness and update the output CSV file\n",
    "calculate_informativeness(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6072c78-fa64-4479-84c0-12383421ee63",
   "metadata": {},
   "source": [
    "## Caculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c90c1b33-73ef-4ff2-8b15-4de085fd416d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(queries):\n",
    "    # Create a TF-IDF vectorizer to convert queries into numerical vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Convert the list of queries to TF-IDF vectors\n",
    "    tfidf_matrix = vectorizer.fit_transform(queries)\n",
    "\n",
    "    # Calculate cosine similarity between all pairs of queries\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "def main():\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    input_csv_file = \"informativness.csv\"  # Replace with the path to your CSV file\n",
    "    output_csv_file = \"queries_with_similarity.csv\"  # Replace with the desired output CSV file\n",
    "\n",
    "    df = pd.read_csv(input_csv_file)\n",
    "\n",
    "    # Get the queries from the DataFrame\n",
    "    queries = df['Query'].tolist()\n",
    "\n",
    "    # Calculate similarity between queries\n",
    "    similarity_matrix = calculate_similarity(queries)\n",
    "\n",
    "    # Add the similarity values as a new column for each query\n",
    "    similarity_column = []\n",
    "    for i in range(len(queries)):\n",
    "        similarity_column.append(similarity_matrix[i].tolist())\n",
    "\n",
    "    df['similarity'] = similarity_column\n",
    "\n",
    "    # Save the DataFrame with the new similarity column to a new CSV file\n",
    "    df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a715f34-3f50-4340-9ed5-7b874287fe93",
   "metadata": {},
   "source": [
    "## Calculate uniqueness of Q(i)\n",
    "\n",
    "We define it as average of the Jaccard similarity(or cosine) scores between the query and all other queries.\n",
    "\n",
    "U(Q(n)) = (Σ J(Q(n), Q(i))) / |Q|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed232eb0-85fd-4028-a7cb-ef817606ceaf",
   "metadata": {},
   "source": [
    "## Jacard similarity and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8098ea-95bd-4959-9acb-74c9be51e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "csv_file_path = 'path_to_your_csv_file.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Step 2: Tokenize the queries\n",
    "tokenized_queries = [query.split() for query in df['sparql_query_column']]\n",
    "\n",
    "# Step 3: Convert queries into sets\n",
    "query_sets = [set(tokens) for tokens in tokenized_queries]\n",
    "\n",
    "# Step 4: Calculate Jaccard similarity\n",
    "jaccard_similarities = 1 - pairwise_distances(query_sets, metric='jaccard')\n",
    "\n",
    "# Step 5: Calculate the average uniqueness score for each query\n",
    "average_uniqueness_scores = jaccard_similarities.mean(axis=1)\n",
    "\n",
    "# Now 'average_uniqueness_scores' contains a single value representing the uniqueness of each query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bbb02-e2b3-44f3-a25a-b014e215e579",
   "metadata": {},
   "source": [
    "## Cosine similarity and vectores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c3edf-b877-4b29-9ff1-66a1866c88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "csv_file_path = 'path_to_your_csv_file.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Step 2: Convert queries into a list of strings\n",
    "query_list = df['sparql_query_column'].tolist()\n",
    "\n",
    "# Step 3: Calculate Cosine similarity\n",
    "vectorizer = CountVectorizer()\n",
    "query_vectors = vectorizer.fit_transform(query_list)\n",
    "cosine_similarities = 1 - pairwise_distances(query_vectors, metric='cosine')\n",
    "\n",
    "# Step 4: Calculate the average uniqueness score for each query\n",
    "average_uniqueness_scores = cosine_similarities.mean(axis=1)\n",
    "\n",
    "# Now 'average_uniqueness_scores' contains a single value representing the uniqueness of each query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c93893e-9197-42c8-b9cf-2dca8adcf6bb",
   "metadata": {},
   "source": [
    "##  Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1af4a8-97a0-4374-93a8-cc0dae43dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "csv_file_path = 'path_to_your_csv_file.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Step 2: Define the batch size for processing\n",
    "batch_size = 1000\n",
    "\n",
    "# Step 3: Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(use_idf=False)\n",
    "\n",
    "# Step 4: Initialize the variable to store the final results\n",
    "result_uniqueness_scores = []\n",
    "\n",
    "# Step 5: Process data in batches\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch_queries = df['sparql_query_column'].iloc[i:i + batch_size].tolist()\n",
    "    # Convert the queries into a sparse matrix\n",
    "    batch_query_vectors = vectorizer.transform(batch_queries)\n",
    "    # Calculate cosine similarity for the batch\n",
    "    cosine_similarities = cosine_similarity(batch_query_vectors)\n",
    "    # Compute the average uniqueness score for each query in the batch\n",
    "    average_uniqueness_scores = cosine_similarities.mean(axis=1)\n",
    "    result_uniqueness_scores.extend(average_uniqueness_scores)\n",
    "\n",
    "# Now 'result_uniqueness_scores' contains a list of values representing the uniqueness of each query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c9386-0c33-4ae6-b9d6-8af1ec7a0b99",
   "metadata": {},
   "source": [
    "## Optimizing parsing of queries\n",
    "\n",
    "we use streams to read and process data in chunks from the input CSV file. We then use the p-map library to parallelize the execution of SPARQL queries using multiple worker threads. This significantly speeds up the processing for large input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce7c48-bdbb-4b67-b91e-e473afc8e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require('fs');\n",
    "const SparqlParser = require('sparqljs').Parser;\n",
    "const csvParser = require('csv-parser');\n",
    "const { unparse } = require('papaparse');\n",
    "const { Worker, isMainThread, parentPort } = require('worker_threads');\n",
    "const pMap = require('p-map');\n",
    "\n",
    "// Create a new instance of the SPARQL parser\n",
    "const parser = new SparqlParser();\n",
    "\n",
    "// Function to execute a single SPARQL query and return the result\n",
    "async function executeQuery(query) {\n",
    "  try {\n",
    "    const parsedQuery = parser.parse(query);\n",
    "    // Here, you can execute the parsed query using your preferred SPARQL endpoint or RDF library\n",
    "    // For the sake of example, we will just return the parsed query as a string\n",
    "    return JSON.stringify(parsedQuery);\n",
    "  } catch (error) {\n",
    "    console.error('Error parsing query:', query);\n",
    "    return 'Error parsing the query.';\n",
    "  }\n",
    "}\n",
    "\n",
    "async function main() {\n",
    "  // Read SPARQL queries from input CSV file and execute them in parallel\n",
    "  const inputCsvFile = 'prefix_unique_bio2rdf_sparql_logs.csv';\n",
    "  const outputCsvFile = 'translated_prefix_unique_bio2rdf_sparql_logs.csv';\n",
    "\n",
    "  const csvData = [];\n",
    "\n",
    "  const workerFunction = async (row) => {\n",
    "    const sparqlQuery = row['query']; // Adjust the column name as per your input CSV format\n",
    "    const parsedQuery = await executeQuery(sparqlQuery);\n",
    "    return { Query: sparqlQuery, Parsed_Query: parsedQuery };\n",
    "  };\n",
    "\n",
    "  // Use pMap to parallelize the execution of queries\n",
    "  const workerCount = 4; // Adjust the number of worker threads as per your CPU cores\n",
    "  const workerResults = await pMap(\n",
    "    fs.createReadStream(inputCsvFile).pipe(csvParser()),\n",
    "    workerFunction,\n",
    "    { concurrency: workerCount }\n",
    "  );\n",
    "\n",
    "  // Add the results to the csvData array\n",
    "  csvData.push(...workerResults);\n",
    "\n",
    "  // Write the results to output CSV file\n",
    "  const csv = unparse(csvData, { header: true, delimiter: ',' });\n",
    "  fs.writeFileSync(outputCsvFile, csv);\n",
    "\n",
    "  console.log('All SPARQL queries executed and results written to output CSV file.');\n",
    "}\n",
    "\n",
    "if (isMainThread) {\n",
    "  main();\n",
    "} else {\n",
    "  parentPort.once('message', (message) => {\n",
    "    parentPort.postMessage(workerFunction(message));\n",
    "  });\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb837aa8-4104-4ae5-8a1f-3ae53777a3a6",
   "metadata": {},
   "source": [
    "## Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ea0b53-526a-45b0-8017-29553ba5b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "const fs = require('fs');\n",
    "const SparqlParser = require('sparqljs').Parser;\n",
    "const csvParser = require('csv-parser');\n",
    "const { unparse } = require('papaparse');\n",
    "const { Worker, isMainThread, parentPort } = require('worker_threads');\n",
    "const pMap = require('p-map');\n",
    "\n",
    "// Create a new instance of the SPARQL parser\n",
    "const parser = new SparqlParser();\n",
    "\n",
    "// Function to execute a single SPARQL query and return the result\n",
    "async function executeQuery(query) {\n",
    "  try {\n",
    "    const parsedQuery = parser.parse(query);\n",
    "    // Here, you can execute the parsed query using your preferred SPARQL endpoint or RDF library\n",
    "    // For the sake of example, we will just return the parsed query as a string\n",
    "    return JSON.stringify(parsedQuery);\n",
    "  } catch (error) {\n",
    "    console.error('Error parsing query:', query);\n",
    "    return 'Error parsing the query.';\n",
    "  }\n",
    "}\n",
    "\n",
    "async function main() {\n",
    "  // Read SPARQL queries from input CSV file and execute them in parallel\n",
    "  const inputCsvFile = 'prefix_unique_bio2rdf_sparql_logs.csv';\n",
    "  const outputCsvFile = 'translated_prefix_unique_bio2rdf_sparql_logs.csv';\n",
    "\n",
    "  const csvData = [];\n",
    "\n",
    "  // Step 1: Count the total number of queries in the input CSV file\n",
    "  let totalQueries = 0;\n",
    "  fs.createReadStream(inputCsvFile)\n",
    "    .pipe(csvParser())\n",
    "    .on('data', () => {\n",
    "      totalQueries++;\n",
    "    })\n",
    "    .on('end', () => {\n",
    "      console.log(`Total SPARQL queries to process: ${totalQueries}`);\n",
    "    });\n",
    "\n",
    "  const workerFunction = async (row) => {\n",
    "    const sparqlQuery = row['query']; // Adjust the column name as per your input CSV format\n",
    "    const parsedQuery = await executeQuery(sparqlQuery);\n",
    "\n",
    "    // Add the results to the csvData array\n",
    "    csvData.push({ Query: sparqlQuery, Parsed_Query: parsedQuery });\n",
    "\n",
    "    // Calculate the completion percentage and show the progress\n",
    "    const completionPercentage = (csvData.length / totalQueries) * 100;\n",
    "    console.log(`Progress: ${completionPercentage.toFixed(2)}%`);\n",
    "\n",
    "    return { Query: sparqlQuery, Parsed_Query: parsedQuery };\n",
    "  };\n",
    "\n",
    "  // Use pMap to parallelize the execution of queries\n",
    "  const workerCount = 4; // Adjust the number of worker threads as per your CPU cores\n",
    "  await pMap(\n",
    "    fs.createReadStream(inputCsvFile).pipe(csvParser()),\n",
    "    workerFunction,\n",
    "    { concurrency: workerCount }\n",
    "  );\n",
    "\n",
    "  // Write the results to output CSV file\n",
    "  const csv = unparse(csvData, { header: true, delimiter: ',' });\n",
    "  fs.writeFileSync(outputCsvFile, csv);\n",
    "\n",
    "  console.log('All SPARQL queries executed and results written to output CSV file.');\n",
    "}\n",
    "\n",
    "if (isMainThread) {\n",
    "  main();\n",
    "} else {\n",
    "  parentPort.once('message', (message) => {\n",
    "    parentPort.postMessage(workerFunction(message));\n",
    "  });\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
